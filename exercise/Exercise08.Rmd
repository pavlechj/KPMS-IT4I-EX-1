---
title: "Exercise 8"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Use crossvalidation and MPI parallelism to optimize `pct` in `mnist/mnist_svd_mv.R`

`pct` is the parameter that controls the percentage of variability captured by the SVD basis functions for each image kind. Currently, this is set at 95% and the resulting test data classification error is a little under 9% (proportion correct 0.9121).

The random forest code runs an error rate under 3% on the MNIST data (see updated `mnist/mnist_rf.R` and `mnist/mnist_rf.sh`). Can we optimize the SVD model to get close?

The crossvalidation should be done on the `svdmod()` function call by dividing the *train* data into 10 random folds, training on 9 folds and predicting (with `predict_svdmod()`) the fold left out, just like we did in the `rf_cv_mc.r` code. Find the optimum among `pct = seq(85, 95, 0.2)` values. This gives 1000 independent computations for parallel treatment. 

I will have more direction later but at first, limit your MPI computation to 2 nodes. We will discuss more MPI concepts and examples in the March 6 Lecture 8.

Due date is Thursday, April 21 (AoE). As last time, each group name your R code in your ASwR repository as EX8.R and its shell script as EX8.sh. Email me at *gostrouc@utk.edu* which repository represents your group.

#### Notes:

A crossvalidation code with mclapply was added: `mpi/mnist_svd_cv.R` and its qsub script `mpi/mnist_svd_cv.sh`. This should be helpful for developing an MPI version. In fact, none of the `function()` definitions need to change. This mclapply crossvalidation runs about 10 minutes on one node of Karolina.

**Some advice for developing the MPI version:**

* All copies of an MPI SPMD code run asynchronously, cooperating via communication (usually collectives).  
* We are reading all the data on every MPI rank. This means that at most about 32 MPI ranks can run on one node before running out of memory.  
* The -qexp queue is limited to 2 nodes, so you can get at most 64 MPI ranks.  
* If you keep mclapply in the code (as opposed to just apply), remember to scale back its core use so that cores are not over subscribed. Each rank will use that many cores.  
* Using MPI together with mclapply (fork) requires these two lines in your submission script (also present in `hello_world_pbs.sh`):  
   * `export OMPI_MCA_mpi_warn_on_fork=0`  
   * `export RDMAV_FORK_SAFE=1`  
* The following line is not necessary, but it avoids a warning message if used:  
   * `module swap libfabric/1.12.1-GCCcore-10.3.0 libfabric/1.13.2-GCCcore-11.2.0`  
* Check out the short codes in `mpi_scripts`, in particular:  
   * The `gather*.r` codes  
   * `chunk.r` code  
   * Run these short scripts with `mpirun -np 3 Rscript short-code.r` on the login node after you `module load R`
* If you are plotting, only one rank should do it for a given output file. That is, put the plot inside `if(comm.rank() == #) { your plot code}`. 
   
**Debugging Tips:**  

* Run smaller instances while debugging. For example:  
    * `folds = 2`  
    * `pars = seq(80, 95, 5)`  
* Print comments to know how far each rank code gets:  
    * `cat(comm.rank(), "Just before function X", "\n")`  
* Print variable values for all ranks (if not too large):  
    * `comm.cat("var1", var1, "\n", all.rank = TRUE)`  
    * `comm.print(object1, all.rank = TRUE)`  
* Print variable properties:  
    * `comm.cat("class(var1)", class(var1), "\n")`  
    * `comm.cat("dim(var1)", dim(var1), "\n")`  
* Remove prints in a section that runs fine so you don't get overwhelmed with output.

**Notes from Questions:**

**Note1**  
A question came up that notes the above `pars` combination ends up with too few instances for parallelism and fails in the `apply` function due to empty combinations.

To expose more parallel opportunity, use both folds and par combinations, which are all independent computations. So chunk up all the rows of cv and not only par, as follows:

my_index = comm.chunk(nrow(cv), form = "vector")

Do this after the cv data frame is formed with expand.grid(). The data frame has two columns named “par” and “fold”, which the function fold_err will access correctly, when given my_index.

For this exercise we’ll just make sure all ranks are getting work. But in general, when there is a possibility of not getting work, the code needs to anticipate this and handle it without failing. We can ignore this complication for now.

**Note2**  
Be aware what objects are replicated and what objects are distributed (local) among the ranks. I often add the `my_` prefix to the local objects to remind me of this.

In tabulating the fold errors together with `tapply()`, this can be done after the `allgather()` or before the `allgather()`:

Before:  
 `my_cv_err_par = tapply(unlist(cv_err), cv[my_index, "par"], sum)`  
 `cv_err_par = allgather(my_cv_err_par)`  
 
After:  
 `cv_err = allgather(my_cv_err)`  
 `cv_err_par = tapply(unlist(cv_err), cv[, "par"], sum)`  

Before the `allgather()`, the tabulation needs to refer only to the local parts of `cv`, so you must subset the replicated `cv` with `my_index`.

But the "before" solution has one more wrinkle. The resulting `cv_err_par` can have duplicate `par` values that need further handling. The "after" solution does not have this issue.

**Note3**  
By default, FlexiBLAS backend is OpenBLAS, which is great as that is the fast library. The default number of threads on Karolina's compute nodes appears to be 16, which is not bad for our application so it is okay to leave it alone. But we did find that for the specific application our optimum was 4 threads so taking it down a little might be beneficial. If you happen to blend mclapply fork together with MPI and this multithreading, you might even find that taking it down to 1 gives a slightly better result.
